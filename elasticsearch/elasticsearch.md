# Elasticsearch - The Definitive Guide

## Overview

* Kibana
    * http://localhost:5601
    * Web UI for elasticsearch

```
Relational DB  ⇒ Databases ⇒ Tables ⇒ Rows      ⇒ Columns
Elasticsearch  ⇒ Indices   ⇒ Types  ⇒ Documents ⇒ Fields
```

### Installing (macOS)

```
brew install elasticsearch kibana logstash
kibana-plugin install x-pack
```

## Examples

```bash
curl -XPOST "http://localhost:9200/_shutdown"
# does not seem to work?


curl -XGET "http://localhost:9200/?pretty"
# {
#   "name": "FNAhp-l",
#   "cluster_name": "elasticsearch_eoinkelly",
#   "cluster_uuid": "5E5Aa-tVQxeMkb9SUQAtqA",
#   "version": {
#     "number": "5.5.0",
#     "build_hash": "260387d",
#     "build_date": "2017-06-30T23:16:05.735Z",
#     "build_snapshot": false,
#     "lucene_version": "6.6.0"
#   },
#   "tagline": "You Know, for Search"
# }


curl -XGET "http://localhost:9200/_cluster/health"
# {
#   "cluster_name": "elasticsearch_eoinkelly",
#   "status": "yellow",
#   "timed_out": false,
#   "number_of_nodes": 1,
#   "number_of_data_nodes": 1,
#   "active_primary_shards": 30,
#   "active_shards": 30,
#   "relocating_shards": 0,
#   "initializing_shards": 0,
#   "unassigned_shards": 30,
#   "delayed_unassigned_shards": 0,
#   "number_of_pending_tasks": 0,
#   "number_of_in_flight_fetch": 0,
#   "task_max_waiting_in_queue_millis": 0,
#   "active_shards_percent_as_number": 50
# }
```

# Getting started

* A node is an instance of ES
* node exposes Restful JSON API on port 9200
* nodes form a cluster on port 9300 (also what the java APIs use)
* is a document database
    * documents have fields
* by default every field in a document is indexed in an inverted index i.e. it is searchable
* uses JSON as the serialization format for documents
* storing a document in ES is called "indexing the document"
* A cluser is a group of nodes with the same vale of `cluster_name` (you can see this value via http://localhost:9200/)
* Java API has two clients
    1. Node client
        * your client joins the cluster as a "non data" node
        * it can forward requests to the node which does have the data
        * uses port 9300 and the native ES transport protocol
    1. Transport client
        * lighter weight
        * just forwards requests to the cluster
        * uses port 9300 and the native ES transport protocol

```
curl http://localhost:9200/
curl http://localhost:9200/_count
curl http://localhost:9200/_cluster/health

```

### Add a document

* use PUT when you have a document id you want to use.
* use POST to autogenerate a document id
    * Autogenerated IDs are 20 character long, URL-safe, Base64-encoded GUID strings.

The form of the URL is

```
PUT {index name}/{type name}/{document id}
POST {index name}/{type name}
```

Examples

```bash
# will autogenerate an document ID
POST /website/blog/
{
    "title": "My second blog entry",
    "text":  "Still trying this out...",
    "date":  "2014/01/01"
}

curl -XPUT 'localhost:9200/megacorp/employee/1?pretty' -H 'Content-Type: application/json' -d'
{
    "first_name" : "John",
    "last_name" :  "Smith",
    "age" :        25,
    "about" :      "I love to go rock climbing",
    "interests": [ "sports", "music" ]
}
'

# response
{
  "_index": "megacorp",
  "_type": "employee",
  "_id": "1",
  "_version": 1,
  "result": "created",
  "_shards": {
    "total": 2,
    "successful": 1,
    "failed": 0
  },
  "_seq_no": 0,
  "_primary_term": 1
}

```

### Retrieve a document

Form is GET /{index name}/{type name}/{document id}

```bash
curl -XGET "http://elasticsearch:9200/megacorp/employee/1"

{
  "_index": "megacorp",
  "_type": "employee",
  "_id": "1",
  "_version": 1,
  "found": true,
  "_source": {
    "first_name": "John",
    "last_name": "Smith",
    "age": 25,
    "about": "I love to go rock climbing",
    "interests": [
      "sports",
      "music"
    ]
  }
}
```

### Search within an index and type

* returns 10 results by default
* Form is `GET /{index name}/{type name}/_search`

```bash
GET /megacorp/employee/_search # "search lite" - return all documents of given index and type

GET /megacorp/employee/_search?q=last_name:Smith # filter those documents based on a field

# exaclty same as line above (but using the query DSL)
# notice this is a GET request with a body. ES authors are fine with this.
GET /megacorp/employee/_search
{
    "query" : {
        "match" : {
            "last_name" : "Smith"
        }
    }
}
```

Examples

```bash

curl -XGET "http://elasticsearch:9200/megacorp/employee/_search"
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 3,
    "max_score": 1,
    "hits": [
      {
        "_index": "megacorp",
        "_type": "employee",
        "_id": "2",
        "_score": 1,
        "_source": {
          "first_name": "Jane",
          "last_name": "Smith",
          "age": 32,
          "about": "I like to collect rock albums",
          "interests": [
            "music"
          ]
        }
      },
      {
        "_index": "megacorp",
        "_type": "employee",
        "_id": "1",
        "_score": 1,
        "_source": {
          "first_name": "John",
          "last_name": "Smith",
          "age": 25,
          "about": "I love to go rock climbing",
          "interests": [
            "sports",
            "music"
          ]
        }
      },
      {
        "_index": "megacorp",
        "_type": "employee",
        "_id": "3",
        "_score": 1,
        "_source": {
          "first_name": "Douglas",
          "last_name": "Fir",
          "age": 35,
          "about": "I like to build cabinets",
          "interests": [
            "forestry"
          ]
        }
      }
    ]
  }
}
```

Searching

```bash
# this is the long-hand version of the '/search' i.e. it finds all results
GET /_search
{
    "query": {
        "match_all": {}
    }
}
```

* Leaf clause
  * used to compare a field against the query string
  * examples:
    * match
    * match_all
    * range
    * term
    * terms
    * exists
    * missing
* Compound clause
  * used to combine other query clauses (both leaf and other compound clauses)
  * examples:
    * `bool`

#### match_all
{ "match_all": {}} # match all documents
    * all resutls receive a neutral score of `1` because they are all equally relevant

#### match
    * good for full text search OR exact value search
    * how it functions depends on they type of the field
        * full text field => use the defined _analyzer_ for that field
        * exact field e.g. numger, date, boolean, _not_analyzed_ string => do an exact match
            * NOTE: for exact matches you probably want a filter clause instead because it will be faster and cached

    { "match": { "fieldName": "field value" }}

    { "match": { "tweet": "About Search" }}
    { "match": { "age":    26           }}
    { "match": { "date":   "2014-09-01" }}
    { "match": { "public": true         }}
    { "match": { "tag":    "full_text"  }}

#### multi_match

* run the same query on multiple fields

{
    "multi_match": {
        "query":    "full text search",
        "fields":   [ "title", "body" ]
    }
}

#### range

{
    "range": {
        "age": {
            "gte":  20,
            "lt":   30
        }
    }
}

#### term

* search by exact value
* `term` query does no analysis of text (does not run any analyzer) so will always find exact match

    { "term": { "age":    26           }}
    { "term": { "date":   "2014-09-01" }}
    { "term": { "public": true         }}
    { "term": { "tag":    "full_text"  }}

#### terms

* same as `term` but allows multiple exact match values
* can be used to do searches for accented and unaccented values

    { "terms": { "tag": [ "search", "full_text", "nosql" ] }}

#### exists

* roughly equivalent to SQL IS NOT NULL
* matches if the given field is not `null` in the document

    {
        "exists":   {
            "field":    "title"
        }
    }

#### missing

* roughly equivalent to SQL IS NULL
* matches if the given field is `null` in the document

    {
        "missing":   {
            "field":    "title"
        }
    }

#### bool

* combines other queryies
* takes the following arguments:
    * must
        * clauses which must match for the document to be included
    * must_not
        * clauses which must not match for the document to be included
    * should
        * if these queries matches we increase the score
        * If there are no must clauses, at least one should clause has to match. However, if there is at least one must clause, no should clauses are required to match.
    * filter
        * these clauses **must** match but don't contribute to the score
        * useful when you don't want a particular criteria to contribute to the score
* a `bool` query can be nested within a `filter` of another `bool`
* combines scores together to return a single score

Queries can be used in two contexts

1. filtering context: does this query match yes/no
    * aka "non scoring" query
    * faster than scoring queries
2. query context: how well does this query match our documents?
    * aka "scoring query"
    * calculates how relevant each document is to the given query and gives it a `_score` which is then used to sort the results by relevance

You can combine both kinds of query for best performnace. First filter out the documents you don't want and then use scoring query to calculate relevance of that subset.

### Types

A type consists of

1. a name
2. a mapping
    * describes the fields/properties that this object may have (i.e. defines a schema for the object)



### Analyzers

An analyzer is a wrapper for a sequence of 3 functions:

1. Character filter (0 or more)
    * tidy up a string before it is tokenized
2. Tokenizer (exactly 1)
    * tokenize the string
3. Token filter (0 or more)
    * filters the tokens in various ways

Available character filters

* `mapping`
    * replace character strings within the stream
* `html_strip`
    * remove html tags

Available Tokenizers

* `standard`
    * splits input text on word boundaries and removes most punctuation
* `keyword`
    * outputs exactlty the string it received
* `whitespace`
    * splits text on whitespace only
* `pattern`
    * split text on a matching regex

Available token filters

* `lowercase`
    * convert token to lowercase
* `stop`
    * removes "stop words" i.e. common words which have little impact on search relevance e.g. the, and, is, an
* `asciifolding`
    * removes diacritics
* `ngram`
    * suitable for partial matching or autocomplete
* `edge_ngram`
    * suitable for partial matching or autocomplete
* `truncate`
    * truncate long tokens

Example: Default analyzer

The default analyzer for full-text fields is `standard` - it is good for western languages.

The `standard` analyzer is

1. Character filter
    * none
1. Tokenizer
    1. `standard` tokenizer
        * splits input text on word boundaries and removes most punctuation
1. Token filters
    1. `standard` token filter
        * in theory tidies up tokens emitted from the tokenizer but currently does nothing
    1. `lowercase` token filter
        * converts all tokens to lowercase
    1. `stop` token filter
        * removes "stop words" i.e. common words which have little impact on search relevance e.g. the, and, is, an


You can pass a test string to a named analyzer within your index:

```
GET http://localhost:9200/myindex/_analyze
{
  "analyzer": "my_analyzer",
  "text":"El veloz zorro marrón"
}
```
This will tell you what that analyzer would do with that string.

# Chapter 2
# Chapter 3
# Chapter 4
# Chapter 5
# Chapter 6 - Mapping & Analysis


# Chapter 7
# Chapter 8
